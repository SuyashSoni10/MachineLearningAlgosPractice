{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load your dataset\\ndf = pd.read_csv(\\'your_financial_data.csv\\')\\n\\n# Create augmentor\\naugmentor = FinancialDataAugmentor(df)\\n\\n# Augment dataset to 2000 samples using mixed methods\\naugmented_data = augmentor.augment_dataset(target_size=2000, method=\\'mixed\\')\\n\\n# Compare statistics\\naugmentor.get_statistics_comparison(augmented_data)\\n\\n# Save augmented dataset\\naugmented_data.to_csv(\\'augmented_financial_data.csv\\', index=False)\\n\\n# Now you can create better train/test splits\\nfrom sklearn.model_selection import train_test_split\\n\\n# Create train/test split (80/20)\\ntrain_data, test_data = train_test_split(augmented_data, test_size=0.2, random_state=42)\\n\\nprint(f\"Training set size: {len(train_data)}\")\\nprint(f\"Test set size: {len(test_data)}\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FinancialDataAugmentor:\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initialize the augmentor with the original dataset\n",
    "        \n",
    "        Parameters:\n",
    "        df: pandas DataFrame with columns ['Date', 'Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
    "        \"\"\"\n",
    "        self.original_df = df.copy()\n",
    "        self.df = df.copy()\n",
    "        self.price_columns = ['Adj Close', 'Close', 'High', 'Low', 'Open']\n",
    "        self.volume_column = 'Volume'\n",
    "        self.date_column = 'Date'\n",
    "        \n",
    "        # Calculate statistics for synthetic generation\n",
    "        self._calculate_statistics()\n",
    "    \n",
    "    def _calculate_statistics(self):\n",
    "        \"\"\"Calculate statistical properties of the original data\"\"\"\n",
    "        # Price statistics\n",
    "        self.price_stats = {}\n",
    "        for col in self.price_columns:\n",
    "            self.price_stats[col] = {\n",
    "                'mean': self.df[col].mean(),\n",
    "                'std': self.df[col].std(),\n",
    "                'min': self.df[col].min(),\n",
    "                'max': self.df[col].max()\n",
    "            }\n",
    "        \n",
    "        # Volume statistics\n",
    "        self.volume_stats = {\n",
    "            'mean': self.df[self.volume_column].mean(),\n",
    "            'std': self.df[self.volume_column].std(),\n",
    "            'min': self.df[self.volume_column].min(),\n",
    "            'max': self.df[self.volume_column].max()\n",
    "        }\n",
    "        \n",
    "        # Calculate daily returns and volatility\n",
    "        self.df['returns'] = self.df['Close'].pct_change()\n",
    "        self.returns_mean = self.df['returns'].mean()\n",
    "        self.returns_std = self.df['returns'].std()\n",
    "        \n",
    "        # Calculate correlation matrix for price relationships\n",
    "        self.price_corr = self.df[self.price_columns].corr()\n",
    "    \n",
    "    def noise_injection(self, noise_factor=0.02, samples=500):\n",
    "        \"\"\"\n",
    "        Add Gaussian noise to existing data points\n",
    "        \n",
    "        Parameters:\n",
    "        noise_factor: float, standard deviation of noise as fraction of data std\n",
    "        samples: int, number of synthetic samples to generate\n",
    "        \"\"\"\n",
    "        synthetic_data = []\n",
    "        \n",
    "        for _ in range(samples):\n",
    "            # Randomly select a base row\n",
    "            base_idx = random.randint(0, len(self.df) - 1)\n",
    "            base_row = self.df.iloc[base_idx].copy()\n",
    "            \n",
    "            # Add noise to price columns\n",
    "            for col in self.price_columns:\n",
    "                noise = np.random.normal(0, self.price_stats[col]['std'] * noise_factor)\n",
    "                base_row[col] = max(0, base_row[col] + noise)  # Ensure positive prices\n",
    "            \n",
    "            # Add noise to volume\n",
    "            volume_noise = np.random.normal(0, self.volume_stats['std'] * noise_factor)\n",
    "            base_row[self.volume_column] = max(1, int(base_row[self.volume_column] + volume_noise))\n",
    "            \n",
    "            # Ensure price relationships are maintained (High >= Low, etc.)\n",
    "            base_row = self._fix_price_relationships(base_row)\n",
    "            \n",
    "            synthetic_data.append(base_row)\n",
    "        \n",
    "        return pd.DataFrame(synthetic_data)\n",
    "    \n",
    "    def interpolation_augmentation(self, samples=300):\n",
    "        \"\"\"\n",
    "        Generate synthetic data by interpolating between existing data points\n",
    "        \"\"\"\n",
    "        synthetic_data = []\n",
    "        \n",
    "        for _ in range(samples):\n",
    "            # Select two random rows\n",
    "            idx1, idx2 = random.sample(range(len(self.df)), 2)\n",
    "            row1 = self.df.iloc[idx1]\n",
    "            row2 = self.df.iloc[idx2]\n",
    "            \n",
    "            # Random interpolation weight\n",
    "            alpha = random.uniform(0.2, 0.8)\n",
    "            \n",
    "            new_row = row1.copy()\n",
    "            \n",
    "            # Interpolate price columns\n",
    "            for col in self.price_columns:\n",
    "                new_row[col] = alpha * row1[col] + (1 - alpha) * row2[col]\n",
    "            \n",
    "            # Interpolate volume\n",
    "            new_row[self.volume_column] = int(alpha * row1[self.volume_column] + (1 - alpha) * row2[self.volume_column])\n",
    "            \n",
    "            # Fix price relationships\n",
    "            new_row = self._fix_price_relationships(new_row)\n",
    "            \n",
    "            synthetic_data.append(new_row)\n",
    "        \n",
    "        return pd.DataFrame(synthetic_data)\n",
    "    \n",
    "    def bootstrap_sampling(self, samples=400):\n",
    "        \"\"\"\n",
    "        Generate synthetic data using bootstrap sampling with slight modifications\n",
    "        \"\"\"\n",
    "        synthetic_data = []\n",
    "        \n",
    "        for _ in range(samples):\n",
    "            # Bootstrap sample\n",
    "            base_row = self.df.sample(n=1).iloc[0].copy()\n",
    "            \n",
    "            # Add small random variations\n",
    "            for col in self.price_columns:\n",
    "                variation = np.random.normal(0, self.price_stats[col]['std'] * 0.01)\n",
    "                base_row[col] = max(0, base_row[col] + variation)\n",
    "            \n",
    "            # Volume variation\n",
    "            volume_variation = np.random.normal(0, self.volume_stats['std'] * 0.01)\n",
    "            base_row[self.volume_column] = max(1, int(base_row[self.volume_column] + volume_variation))\n",
    "            \n",
    "            # Fix price relationships\n",
    "            base_row = self._fix_price_relationships(base_row)\n",
    "            \n",
    "            synthetic_data.append(base_row)\n",
    "        \n",
    "        return pd.DataFrame(synthetic_data)\n",
    "    \n",
    "    def trend_based_generation(self, samples=350):\n",
    "        \"\"\"\n",
    "        Generate synthetic data based on historical trends and patterns\n",
    "        \"\"\"\n",
    "        synthetic_data = []\n",
    "        \n",
    "        # Calculate moving averages for trend\n",
    "        self.df['ma_5'] = self.df['Close'].rolling(window=5).mean()\n",
    "        self.df['ma_20'] = self.df['Close'].rolling(window=20).mean()\n",
    "        \n",
    "        for _ in range(samples):\n",
    "            # Select a base period\n",
    "            base_idx = random.randint(20, len(self.df) - 1)\n",
    "            base_row = self.df.iloc[base_idx].copy()\n",
    "            \n",
    "            # Calculate trend direction\n",
    "            trend = self.df.iloc[base_idx]['ma_5'] - self.df.iloc[base_idx]['ma_20']\n",
    "            trend_factor = np.tanh(trend / self.df.iloc[base_idx]['Close'])  # Normalize trend\n",
    "            \n",
    "            # Generate new prices based on trend\n",
    "            for col in self.price_columns:\n",
    "                base_price = base_row[col]\n",
    "                trend_adjustment = base_price * trend_factor * random.uniform(-0.02, 0.02)\n",
    "                noise = np.random.normal(0, self.price_stats[col]['std'] * 0.015)\n",
    "                base_row[col] = max(0, base_price + trend_adjustment + noise)\n",
    "            \n",
    "            # Volume adjustment based on price movement\n",
    "            price_change = (base_row['Close'] - base_row['Open']) / base_row['Open']\n",
    "            volume_multiplier = 1 + abs(price_change) * random.uniform(0.1, 0.3)\n",
    "            base_row[self.volume_column] = int(base_row[self.volume_column] * volume_multiplier)\n",
    "            \n",
    "            # Fix price relationships\n",
    "            base_row = self._fix_price_relationships(base_row)\n",
    "            \n",
    "            synthetic_data.append(base_row)\n",
    "        \n",
    "        return pd.DataFrame(synthetic_data)\n",
    "    \n",
    "    def _fix_price_relationships(self, row):\n",
    "        \"\"\"\n",
    "        Ensure price relationships are maintained (High >= Close >= Low, etc.)\n",
    "        \"\"\"\n",
    "        prices = [row['Open'], row['Close'], row['High'], row['Low']]\n",
    "        \n",
    "        # Set High as maximum of all prices\n",
    "        row['High'] = max(prices)\n",
    "        \n",
    "        # Set Low as minimum of all prices\n",
    "        row['Low'] = min(prices)\n",
    "        \n",
    "        # Ensure Adj Close is reasonable relative to Close\n",
    "        if abs(row['Adj Close'] - row['Close']) > row['Close'] * 0.1:\n",
    "            row['Adj Close'] = row['Close'] + np.random.normal(0, row['Close'] * 0.005)\n",
    "        \n",
    "        return row\n",
    "    \n",
    "    def generate_synthetic_dates(self, num_samples):\n",
    "        \"\"\"\n",
    "        Generate synthetic dates for the augmented data\n",
    "        \"\"\"\n",
    "        # Convert date column to datetime if it's not already\n",
    "        if self.df[self.date_column].dtype == 'object':\n",
    "            self.df[self.date_column] = pd.to_datetime(self.df[self.date_column])\n",
    "        \n",
    "        # Find date range\n",
    "        start_date = self.df[self.date_column].min()\n",
    "        end_date = self.df[self.date_column].max()\n",
    "        \n",
    "        # Generate random dates within the range\n",
    "        time_range = (end_date - start_date).days\n",
    "        synthetic_dates = []\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            random_days = random.randint(0, time_range)\n",
    "            synthetic_date = start_date + timedelta(days=random_days)\n",
    "            synthetic_dates.append(synthetic_date)\n",
    "        \n",
    "        return synthetic_dates\n",
    "    \n",
    "    def augment_dataset(self, target_size=2000, method='mixed'):\n",
    "        \"\"\"\n",
    "        Main method to augment the dataset\n",
    "        \n",
    "        Parameters:\n",
    "        target_size: int, desired size of the augmented dataset\n",
    "        method: str, augmentation method ('mixed', 'noise', 'interpolation', 'bootstrap', 'trend')\n",
    "        \"\"\"\n",
    "        samples_needed = target_size - len(self.original_df)\n",
    "        \n",
    "        if samples_needed <= 0:\n",
    "            print(\"Dataset is already larger than target size!\")\n",
    "            return self.original_df\n",
    "        \n",
    "        print(f\"Generating {samples_needed} synthetic samples...\")\n",
    "        \n",
    "        synthetic_dfs = []\n",
    "        \n",
    "        if method == 'mixed':\n",
    "            # Use all methods\n",
    "            noise_samples = int(samples_needed * 0.3)\n",
    "            interp_samples = int(samples_needed * 0.25)\n",
    "            bootstrap_samples = int(samples_needed * 0.25)\n",
    "            trend_samples = samples_needed - noise_samples - interp_samples - bootstrap_samples\n",
    "            \n",
    "            print(f\"Noise injection: {noise_samples} samples\")\n",
    "            synthetic_dfs.append(self.noise_injection(samples=noise_samples))\n",
    "            \n",
    "            print(f\"Interpolation: {interp_samples} samples\")\n",
    "            synthetic_dfs.append(self.interpolation_augmentation(samples=interp_samples))\n",
    "            \n",
    "            print(f\"Bootstrap sampling: {bootstrap_samples} samples\")\n",
    "            synthetic_dfs.append(self.bootstrap_sampling(samples=bootstrap_samples))\n",
    "            \n",
    "            print(f\"Trend-based generation: {trend_samples} samples\")\n",
    "            synthetic_dfs.append(self.trend_based_generation(samples=trend_samples))\n",
    "            \n",
    "        elif method == 'noise':\n",
    "            synthetic_dfs.append(self.noise_injection(samples=samples_needed))\n",
    "        elif method == 'interpolation':\n",
    "            synthetic_dfs.append(self.interpolation_augmentation(samples=samples_needed))\n",
    "        elif method == 'bootstrap':\n",
    "            synthetic_dfs.append(self.bootstrap_sampling(samples=samples_needed))\n",
    "        elif method == 'trend':\n",
    "            synthetic_dfs.append(self.trend_based_generation(samples=samples_needed))\n",
    "        \n",
    "        # Combine all synthetic data\n",
    "        all_synthetic = pd.concat(synthetic_dfs, ignore_index=True)\n",
    "        \n",
    "        # Generate synthetic dates\n",
    "        synthetic_dates = self.generate_synthetic_dates(len(all_synthetic))\n",
    "        all_synthetic[self.date_column] = synthetic_dates\n",
    "        \n",
    "        # Combine original and synthetic data\n",
    "        augmented_df = pd.concat([self.original_df, all_synthetic], ignore_index=True)\n",
    "        \n",
    "        # Shuffle the dataset\n",
    "        augmented_df = augmented_df.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Dataset augmented from {len(self.original_df)} to {len(augmented_df)} samples\")\n",
    "        \n",
    "        return augmented_df\n",
    "    \n",
    "    def get_statistics_comparison(self, augmented_df):\n",
    "        \"\"\"\n",
    "        Compare statistics between original and augmented datasets\n",
    "        \"\"\"\n",
    "        print(\"\\n=== STATISTICS COMPARISON ===\")\n",
    "        print(f\"Original dataset size: {len(self.original_df)}\")\n",
    "        print(f\"Augmented dataset size: {len(augmented_df)}\")\n",
    "        \n",
    "        print(\"\\nPrice Statistics Comparison:\")\n",
    "        for col in self.price_columns:\n",
    "            orig_mean = self.original_df[col].mean()\n",
    "            orig_std = self.original_df[col].std()\n",
    "            aug_mean = augmented_df[col].mean()\n",
    "            aug_std = augmented_df[col].std()\n",
    "            \n",
    "            print(f\"{col}:\")\n",
    "            print(f\"  Original - Mean: {orig_mean:.2f}, Std: {orig_std:.2f}\")\n",
    "            print(f\"  Augmented - Mean: {aug_mean:.2f}, Std: {aug_std:.2f}\")\n",
    "            print(f\"  Difference - Mean: {abs(orig_mean - aug_mean):.2f}, Std: {abs(orig_std - aug_std):.2f}\")\n",
    "            print()\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Load your dataset\n",
    "df = pd.read_csv('your_financial_data.csv')\n",
    "\n",
    "# Create augmentor\n",
    "augmentor = FinancialDataAugmentor(df)\n",
    "\n",
    "# Augment dataset to 2000 samples using mixed methods\n",
    "augmented_data = augmentor.augment_dataset(target_size=2000, method='mixed')\n",
    "\n",
    "# Compare statistics\n",
    "augmentor.get_statistics_comparison(augmented_data)\n",
    "\n",
    "# Save augmented dataset\n",
    "augmented_data.to_csv('augmented_financial_data.csv', index=False)\n",
    "\n",
    "# Now you can create better train/test splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create train/test split (80/20)\n",
    "train_data, test_data = train_test_split(augmented_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1748 synthetic samples...\n",
      "Noise injection: 524 samples\n",
      "Interpolation: 437 samples\n",
      "Bootstrap sampling: 437 samples\n",
      "Trend-based generation: 350 samples\n",
      "Dataset augmented from 252 to 2000 samples\n",
      "\n",
      "=== STATISTICS COMPARISON ===\n",
      "Original dataset size: 252\n",
      "Augmented dataset size: 2000\n",
      "\n",
      "Price Statistics Comparison:\n",
      "Adj Close:\n",
      "  Original - Mean: 199.09, Std: 21.51\n",
      "  Augmented - Mean: 198.74, Std: 20.35\n",
      "  Difference - Mean: 0.35, Std: 1.16\n",
      "\n",
      "Close:\n",
      "  Original - Mean: 199.45, Std: 21.32\n",
      "  Augmented - Mean: 199.11, Std: 20.17\n",
      "  Difference - Mean: 0.35, Std: 1.16\n",
      "\n",
      "High:\n",
      "  Original - Mean: 201.09, Std: 21.57\n",
      "  Augmented - Mean: 200.76, Std: 20.32\n",
      "  Difference - Mean: 0.33, Std: 1.25\n",
      "\n",
      "Low:\n",
      "  Original - Mean: 197.61, Std: 20.96\n",
      "  Augmented - Mean: 197.24, Std: 19.80\n",
      "  Difference - Mean: 0.37, Std: 1.16\n",
      "\n",
      "Open:\n",
      "  Original - Mean: 199.32, Std: 21.37\n",
      "  Augmented - Mean: 199.00, Std: 20.17\n",
      "  Difference - Mean: 0.31, Std: 1.19\n",
      "\n",
      "Training set size: 1600\n",
      "Test set size: 400\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('D:\\\\ML PROJECTS\\\\AML lab\\\\dataset\\\\apple_stock.csv')\n",
    "\n",
    "# Create augmentor\n",
    "augmentor = FinancialDataAugmentor(df)\n",
    "\n",
    "# Augment dataset to 2000 samples using mixed methods\n",
    "augmented_data = augmentor.augment_dataset(target_size=2000, method='mixed')\n",
    "\n",
    "# Compare statistics\n",
    "augmentor.get_statistics_comparison(augmented_data)\n",
    "\n",
    "# Save augmented dataset\n",
    "augmented_data.to_csv('augmented_financial_data.csv', index=False)\n",
    "\n",
    "# Now you can create better train/test splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create train/test split (80/20)\n",
    "train_data, test_data = train_test_split(augmented_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
