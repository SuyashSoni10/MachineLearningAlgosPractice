{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET ANALYSIS FOR 2000 SAMPLES\n",
      "============================================================\n",
      "Original dataset shape: (2000, 10)\n",
      "Missing values per column:\n",
      "  returns: 260 (13.0%)\n",
      "  ma_5: 1650 (82.5%)\n",
      "  ma_20: 1650 (82.5%)\n",
      "\n",
      "Creating enhanced features...\n",
      "Dataset shape after cleaning: (347, 41)\n",
      "Samples lost due to feature engineering: 1653\n",
      "Available features: 24\n",
      "\n",
      "Class distribution:\n",
      "  Down: 0.473 (164 samples)\n",
      "  Sideways: 0.069 (24 samples)\n",
      "  Up: 0.458 (159 samples)\n",
      "\n",
      "============================================================\n",
      "MULTIPLE EVALUATION STRATEGIES\n",
      "============================================================\n",
      "1. Standard 75/25 Time-based Split\n",
      "   Training: 260 samples\n",
      "   Testing: 87 samples\n",
      "\n",
      "2. Current 80/20 Time-based Split\n",
      "   Training: 277 samples\n",
      "   Testing: 70 samples\n",
      "\n",
      "3. Time Series Cross-Validation (5-fold)\n",
      "   Fold 1: Train=62, Test=57, Acc=0.7895\n",
      "   Fold 2: Train=119, Test=57, Acc=0.5088\n",
      "   Fold 3: Train=176, Test=57, Acc=0.5789\n",
      "   Fold 4: Train=233, Test=57, Acc=0.5263\n",
      "   Fold 5: Train=290, Test=57, Acc=0.6140\n",
      "\n",
      "4. Rolling Window (252 train, 63 test)\n",
      "   Valid windows: 1\n",
      "   Window scores: ['0.4286']\n",
      "\n",
      "5. Expanding Window\n",
      "   Valid expansions: 0\n",
      "   Not enough data for expanding window\n",
      "\n",
      "============================================================\n",
      "RESULTS SUMMARY\n",
      "============================================================\n",
      "standard_75_25      : 0.6782\n",
      "current_80_20       : 0.5857\n",
      "time_series_cv      : 0.6035 (±0.1002)\n",
      "time_series_cv_std  : 0.1002\n",
      "rolling_window      : 0.4286\n",
      "expanding_window    : Not applicable\n",
      "\n",
      "============================================================\n",
      "RECOMMENDATIONS FOR 2000 SAMPLES\n",
      "============================================================\n",
      "1. OPTIMAL SPLIT STRATEGY:\n",
      "   - Use 75/25 split (1500 train, 500 test)\n",
      "   - This gives you more test samples for robust evaluation\n",
      "\n",
      "2. CROSS-VALIDATION:\n",
      "   - Use Time Series Cross-Validation with 5 folds\n",
      "   - This gives you multiple evaluation points\n",
      "\n",
      "3. MODEL PARAMETERS:\n",
      "   - max_depth=8 (reduced from 10)\n",
      "   - min_samples_split=10 (reduced from 15)\n",
      "   - min_samples_leaf=5 (reduced from 8)\n",
      "\n",
      "4. FEATURE ENGINEERING:\n",
      "   - Use shorter time windows (3, 5, 10 days)\n",
      "   - Focus on momentum and volatility indicators\n",
      "\n",
      "============================================================\n",
      "DETAILED MODEL ANALYSIS\n",
      "============================================================\n",
      "Test Accuracy: 0.4598\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.69      0.40      0.51        45\n",
      "    Sideways       0.00      0.00      0.00         3\n",
      "          Up       0.63      0.56      0.59        39\n",
      "\n",
      "    accuracy                           0.46        87\n",
      "   macro avg       0.44      0.32      0.37        87\n",
      "weighted avg       0.64      0.46      0.53        87\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "         Down  Sideways  Up\n",
      "    Down: [18 17 10]\n",
      "Sideways: [0 0 3]\n",
      "      Up: [ 8  9 22]\n",
      "\n",
      "Top 10 Most Important Features:\n",
      " 1. Price_Change         0.2611\n",
      " 2. Price_Momentum_2     0.1259\n",
      " 3. RSI                  0.1048\n",
      " 4. Volatility_5         0.0939\n",
      " 5. Volatility_10        0.0606\n",
      " 6. High_Low_Ratio       0.0585\n",
      " 7. BB_Position          0.0537\n",
      " 8. High                 0.0505\n",
      " 9. Volume_Momentum_3    0.0415\n",
      "10. Low                  0.0411\n",
      "\n",
      "Trading Strategy Analysis:\n",
      "Confidence >0.5: 80/87 predictions, Accuracy: 0.5000\n",
      "Confidence >0.6: 75/87 predictions, Accuracy: 0.5333\n",
      "Confidence >0.7: 58/87 predictions, Accuracy: 0.6552\n",
      "\n",
      "Class-specific Trading Performance:\n",
      "    Down predictions: 26/87 (29.89%), Accuracy: 0.6923\n",
      "Sideways predictions: 26/87 (29.89%), Accuracy: 0.0000\n",
      "      Up predictions: 35/87 (40.23%), Accuracy: 0.6286\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def optimize_for_2000_samples(df):\n",
    "    \"\"\"\n",
    "    Optimized strategy specifically for 2000 sample financial dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. DATA CLEANING AND PREPARATION\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATASET ANALYSIS FOR 2000 SAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Convert Date and sort\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # Check data quality\n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "    print(f\"Missing values per column:\")\n",
    "    missing_info = df.isnull().sum()\n",
    "    for col, missing in missing_info.items():\n",
    "        if missing > 0:\n",
    "            print(f\"  {col}: {missing} ({missing/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # 2. ENHANCED FEATURE ENGINEERING\n",
    "    print(f\"\\nCreating enhanced features...\")\n",
    "    \n",
    "    # Basic price features\n",
    "    df['Price_Change'] = df['Close'].pct_change()\n",
    "    df['Volume_Change'] = df['Volume'].pct_change()\n",
    "    df['High_Low_Ratio'] = df['High'] / df['Low']\n",
    "    df['Open_Close_Ratio'] = df['Open'] / df['Close']\n",
    "    df['Price_Position'] = (df['Close'] - df['Low']) / (df['High'] - df['Low'])\n",
    "    \n",
    "    # Moving averages (use shorter windows for 2000 samples)\n",
    "    for window in [3, 5, 10, 20]:\n",
    "        df[f'SMA_{window}'] = df['Close'].rolling(window=window).mean()\n",
    "        df[f'Price_Above_SMA{window}'] = (df['Close'] > df[f'SMA_{window}']).astype(int)\n",
    "    \n",
    "    # Volatility indicators\n",
    "    df['Volatility_5'] = df['Price_Change'].rolling(window=5).std()\n",
    "    df['Volatility_10'] = df['Price_Change'].rolling(window=10).std()\n",
    "    \n",
    "    # Momentum indicators (shorter periods for 2000 samples)\n",
    "    for period in [2, 3, 5]:\n",
    "        df[f'Price_Momentum_{period}'] = df['Close'] / df['Close'].shift(period)\n",
    "        df[f'Volume_Momentum_{period}'] = df['Volume'] / df['Volume'].shift(period)\n",
    "    \n",
    "    # Previous day indicators\n",
    "    df['Prev_Day_Up'] = (df['Close'] > df['Close'].shift(1)).astype(int)\n",
    "    df['Prev_Day_Volume_Ratio'] = df['Volume'] / df['Volume'].shift(1)\n",
    "    \n",
    "    # RSI (14-period)\n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Bollinger Bands position\n",
    "    bb_period = 20\n",
    "    df['BB_Middle'] = df['Close'].rolling(window=bb_period).mean()\n",
    "    bb_std = df['Close'].rolling(window=bb_period).std()\n",
    "    df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)\n",
    "    df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)\n",
    "    df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
    "    \n",
    "    # 3. TARGET VARIABLE CREATION\n",
    "    df['Next_Day_Close'] = df['Close'].shift(-1)\n",
    "    df['Price_Change_Pct'] = (df['Next_Day_Close'] - df['Close']) / df['Close']\n",
    "    \n",
    "    # Multi-class with optimized thresholds for 2000 samples\n",
    "    threshold = 0.01  # 1% threshold\n",
    "    df['Price_Direction'] = np.where(df['Price_Change_Pct'] > threshold, 2,\n",
    "                                   np.where(df['Price_Change_Pct'] < -threshold, 0, 1))\n",
    "    \n",
    "    # 4. FEATURE SELECTION\n",
    "    features = [\n",
    "        'Open', 'High', 'Low', 'Volume',\n",
    "        'Price_Change', 'Volume_Change',\n",
    "        'High_Low_Ratio', 'Open_Close_Ratio', 'Price_Position',\n",
    "        'Price_Above_SMA3', 'Price_Above_SMA5', 'Price_Above_SMA10',\n",
    "        'Volatility_5', 'Volatility_10',\n",
    "        'Price_Momentum_2', 'Price_Momentum_3', 'Price_Momentum_5',\n",
    "        'Volume_Momentum_2', 'Volume_Momentum_3', 'Volume_Momentum_5',\n",
    "        'Prev_Day_Up', 'Prev_Day_Volume_Ratio',\n",
    "        'RSI', 'BB_Position'\n",
    "    ]\n",
    "    \n",
    "    # Clean data and prepare features\n",
    "    df_clean = df.dropna()\n",
    "    print(f\"Dataset shape after cleaning: {df_clean.shape}\")\n",
    "    print(f\"Samples lost due to feature engineering: {len(df) - len(df_clean)}\")\n",
    "    \n",
    "    # Select available features\n",
    "    available_features = [f for f in features if f in df_clean.columns]\n",
    "    print(f\"Available features: {len(available_features)}\")\n",
    "    \n",
    "    X = df_clean[available_features]\n",
    "    y = df_clean['Price_Direction']\n",
    "    \n",
    "    # Check class distribution\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    class_dist = y.value_counts(normalize=True).sort_index()\n",
    "    for class_idx, proportion in class_dist.items():\n",
    "        class_name = ['Down', 'Sideways', 'Up'][class_idx]\n",
    "        print(f\"  {class_name}: {proportion:.3f} ({y.value_counts()[class_idx]} samples)\")\n",
    "    \n",
    "    return df_clean, X, y, available_features\n",
    "\n",
    "def multiple_evaluation_strategies(X, y, df_clean):\n",
    "    \"\"\"\n",
    "    Evaluate multiple train/test strategies for 2000 samples\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MULTIPLE EVALUATION STRATEGIES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Model configuration optimized for 2000 samples\n",
    "    model_config = {\n",
    "        'max_depth': 8,              # Reduced from 10\n",
    "        'min_samples_split': 10,     # Reduced from 15\n",
    "        'min_samples_leaf': 5,       # Reduced from 8\n",
    "        'max_features': 'sqrt',\n",
    "        'class_weight': 'balanced',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Strategy 1: Standard 75/25 split (better for 2000 samples)\n",
    "    print(\"1. Standard 75/25 Time-based Split\")\n",
    "    split_index = int(len(X) * 0.75)  # 1500 train, 500 test\n",
    "    \n",
    "    X_train = X[:split_index]\n",
    "    X_test = X[split_index:]\n",
    "    y_train = y[:split_index]\n",
    "    y_test = y[split_index:]\n",
    "    \n",
    "    print(f\"   Training: {len(X_train)} samples\")\n",
    "    print(f\"   Testing: {len(X_test)} samples\")\n",
    "    \n",
    "    model = DecisionTreeClassifier(**model_config)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    results['standard_75_25'] = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Strategy 2: 80/20 split (your current approach)\n",
    "    print(\"\\n2. Current 80/20 Time-based Split\")\n",
    "    split_index = int(len(X) * 0.80)  # 1600 train, 400 test\n",
    "    \n",
    "    X_train_80 = X[:split_index]\n",
    "    X_test_80 = X[split_index:]\n",
    "    y_train_80 = y[:split_index]\n",
    "    y_test_80 = y[split_index:]\n",
    "    \n",
    "    print(f\"   Training: {len(X_train_80)} samples\")\n",
    "    print(f\"   Testing: {len(X_test_80)} samples\")\n",
    "    \n",
    "    model.fit(X_train_80, y_train_80)\n",
    "    y_pred_80 = model.predict(X_test_80)\n",
    "    results['current_80_20'] = accuracy_score(y_test_80, y_pred_80)\n",
    "    \n",
    "    # Strategy 3: Time Series Cross-Validation (5-fold)\n",
    "    print(\"\\n3. Time Series Cross-Validation (5-fold)\")\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    cv_scores = []\n",
    "    \n",
    "    fold_info = []\n",
    "    for i, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "        X_train_cv = X.iloc[train_idx]\n",
    "        X_test_cv = X.iloc[test_idx]\n",
    "        y_train_cv = y.iloc[train_idx]\n",
    "        y_test_cv = y.iloc[test_idx]\n",
    "        \n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "        y_pred_cv = model.predict(X_test_cv)\n",
    "        score = accuracy_score(y_test_cv, y_pred_cv)\n",
    "        cv_scores.append(score)\n",
    "        \n",
    "        fold_info.append({\n",
    "            'fold': i+1,\n",
    "            'train_size': len(train_idx),\n",
    "            'test_size': len(test_idx),\n",
    "            'accuracy': score\n",
    "        })\n",
    "    \n",
    "    for info in fold_info:\n",
    "        print(f\"   Fold {info['fold']}: Train={info['train_size']}, Test={info['test_size']}, Acc={info['accuracy']:.4f}\")\n",
    "    \n",
    "    results['time_series_cv'] = np.mean(cv_scores)\n",
    "    results['time_series_cv_std'] = np.std(cv_scores)\n",
    "    \n",
    "    # Strategy 4: Rolling Window (1 year train, 3 months test)\n",
    "    print(\"\\n4. Rolling Window (252 train, 63 test)\")\n",
    "    window_size = 252  # 1 year\n",
    "    test_size = 63     # 3 months\n",
    "    \n",
    "    rolling_scores = []\n",
    "    valid_windows = 0\n",
    "    \n",
    "    for i in range(window_size, len(X) - test_size + 1, test_size):\n",
    "        if i + test_size <= len(X):\n",
    "            train_start = i - window_size\n",
    "            train_end = i\n",
    "            test_start = i\n",
    "            test_end = i + test_size\n",
    "            \n",
    "            X_train_roll = X.iloc[train_start:train_end]\n",
    "            X_test_roll = X.iloc[test_start:test_end]\n",
    "            y_train_roll = y.iloc[train_start:train_end]\n",
    "            y_test_roll = y.iloc[test_start:test_end]\n",
    "            \n",
    "            model.fit(X_train_roll, y_train_roll)\n",
    "            y_pred_roll = model.predict(X_test_roll)\n",
    "            score = accuracy_score(y_test_roll, y_pred_roll)\n",
    "            rolling_scores.append(score)\n",
    "            valid_windows += 1\n",
    "    \n",
    "    print(f\"   Valid windows: {valid_windows}\")\n",
    "    if rolling_scores:\n",
    "        results['rolling_window'] = np.mean(rolling_scores)\n",
    "        print(f\"   Window scores: {[f'{score:.4f}' for score in rolling_scores]}\")\n",
    "    else:\n",
    "        results['rolling_window'] = None\n",
    "        print(\"   Not enough data for rolling window\")\n",
    "    \n",
    "    # Strategy 5: Expanding Window\n",
    "    print(\"\\n5. Expanding Window\")\n",
    "    initial_train = 500   # Start with 500 samples\n",
    "    test_size = 100       # Test on 100 samples\n",
    "    \n",
    "    expanding_scores = []\n",
    "    valid_expansions = 0\n",
    "    \n",
    "    for i in range(initial_train, len(X) - test_size + 1, test_size):\n",
    "        if i + test_size <= len(X):\n",
    "            X_train_exp = X.iloc[0:i]\n",
    "            X_test_exp = X.iloc[i:i+test_size]\n",
    "            y_train_exp = y.iloc[0:i]\n",
    "            y_test_exp = y.iloc[i:i+test_size]\n",
    "            \n",
    "            model.fit(X_train_exp, y_train_exp)\n",
    "            y_pred_exp = model.predict(X_test_exp)\n",
    "            score = accuracy_score(y_test_exp, y_pred_exp)\n",
    "            expanding_scores.append(score)\n",
    "            valid_expansions += 1\n",
    "    \n",
    "    print(f\"   Valid expansions: {valid_expansions}\")\n",
    "    if expanding_scores:\n",
    "        results['expanding_window'] = np.mean(expanding_scores)\n",
    "        print(f\"   Expansion scores: {[f'{score:.4f}' for score in expanding_scores]}\")\n",
    "    else:\n",
    "        results['expanding_window'] = None\n",
    "        print(\"   Not enough data for expanding window\")\n",
    "    \n",
    "    return results, model, X_train, X_test, y_train, y_test\n",
    "\n",
    "def detailed_analysis(model, X_train, X_test, y_train, y_test, available_features):\n",
    "    \"\"\"\n",
    "    Detailed analysis of the best model\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DETAILED MODEL ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    target_names = ['Down', 'Sideways', 'Up']\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(\"         Down  Sideways  Up\")\n",
    "    for i, row in enumerate(cm):\n",
    "        print(f\"{target_names[i]:>8}: {row}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': available_features,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Most Important Features:\")\n",
    "    for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):\n",
    "        print(f\"{i+1:2d}. {row['feature']:<20} {row['importance']:.4f}\")\n",
    "    \n",
    "    # Trading strategy analysis\n",
    "    print(f\"\\nTrading Strategy Analysis:\")\n",
    "    \n",
    "    # High confidence predictions\n",
    "    max_proba = y_pred_proba.max(axis=1)\n",
    "    \n",
    "    for confidence_threshold in [0.5, 0.6, 0.7]:\n",
    "        high_confidence = max_proba > confidence_threshold\n",
    "        if high_confidence.sum() > 0:\n",
    "            high_conf_accuracy = (y_test[high_confidence] == y_pred[high_confidence]).mean()\n",
    "            print(f\"Confidence >{confidence_threshold:.1f}: {high_confidence.sum()}/{len(y_test)} predictions, Accuracy: {high_conf_accuracy:.4f}\")\n",
    "    \n",
    "    # Class-specific performance\n",
    "    print(f\"\\nClass-specific Trading Performance:\")\n",
    "    for class_idx, class_name in enumerate(target_names):\n",
    "        class_predictions = (y_pred == class_idx)\n",
    "        if class_predictions.sum() > 0:\n",
    "            class_accuracy = (y_test[class_predictions] == class_idx).mean()\n",
    "            print(f\"{class_name:>8} predictions: {class_predictions.sum()}/{len(y_test)} ({class_predictions.mean():.2%}), Accuracy: {class_accuracy:.4f}\")\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Load your dataset\n",
    "    df = pd.read_csv('D:\\\\ML PROJECTS\\\\AML lab\\\\augmented_financial_data.csv')\n",
    "    \n",
    "    # Optimize for 2000 samples\n",
    "    df_clean, X, y, available_features = optimize_for_2000_samples(df)\n",
    "    \n",
    "    # Multiple evaluation strategies\n",
    "    results, best_model, X_train, X_test, y_train, y_test = multiple_evaluation_strategies(X, y, df_clean)\n",
    "    \n",
    "    # Display results summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for strategy, score in results.items():\n",
    "        if score is not None:\n",
    "            if strategy == 'time_series_cv':\n",
    "                print(f\"{strategy:<20}: {score:.4f} (±{results['time_series_cv_std']:.4f})\")\n",
    "            else:\n",
    "                print(f\"{strategy:<20}: {score:.4f}\")\n",
    "        else:\n",
    "            print(f\"{strategy:<20}: Not applicable\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"RECOMMENDATIONS FOR 2000 SAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"1. OPTIMAL SPLIT STRATEGY:\")\n",
    "    print(\"   - Use 75/25 split (1500 train, 500 test)\")\n",
    "    print(\"   - This gives you more test samples for robust evaluation\")\n",
    "    \n",
    "    print(\"\\n2. CROSS-VALIDATION:\")\n",
    "    print(\"   - Use Time Series Cross-Validation with 5 folds\")\n",
    "    print(\"   - This gives you multiple evaluation points\")\n",
    "    \n",
    "    print(\"\\n3. MODEL PARAMETERS:\")\n",
    "    print(\"   - max_depth=8 (reduced from 10)\")\n",
    "    print(\"   - min_samples_split=10 (reduced from 15)\")\n",
    "    print(\"   - min_samples_leaf=5 (reduced from 8)\")\n",
    "    \n",
    "    print(\"\\n4. FEATURE ENGINEERING:\")\n",
    "    print(\"   - Use shorter time windows (3, 5, 10 days)\")\n",
    "    print(\"   - Focus on momentum and volatility indicators\")\n",
    "    \n",
    "    # Detailed analysis\n",
    "    detailed_analysis(best_model, X_train, X_test, y_train, y_test, available_features)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
